{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2bd123b",
   "metadata": {},
   "source": [
    "### PCA in Machine Learning Workflows\n",
    "#### Machine Learning I - Maestría en Analítica Aplicada\n",
    "#### Universidad de la Sabana\n",
    "#### Prof: Hugo Franco\n",
    "#### Example: Principal Component Analysis\n",
    "\n",
    "<img src=\"culmen_depth.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Load the penguins dataset\n",
    "penguins = sns.load_dataset('penguins')\n",
    "\n",
    "# Display initial information\n",
    "print(\"Dataset Overview:\")\n",
    "print(penguins.info())\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(penguins['species'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700161ac",
   "metadata": {},
   "source": [
    "#### Imputation strategies implemented in SimpleImputer \n",
    "* mean (default for numeric data)\n",
    "* median (usually more robust than mean)\n",
    "* most_frequent\n",
    "* constant (requires the filling value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a498393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature groups\n",
    "numeric_features = ['bill_length_mm', 'bill_depth_mm', \n",
    "                   'flipper_length_mm', 'body_mass_g']\n",
    "categorical_features = ['sex', 'island']\n",
    "\n",
    "# Create the numeric transformer\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create the categorical transformer with one-hot encoding\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(\n",
    "        drop='first',  # Drop first category to avoid multicollinearity\n",
    "        sparse_output=False,  # Return dense array instead of sparse matrix\n",
    "        handle_unknown='ignore'  # Handle new categories in test data\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Print encoded feature names\n",
    "def get_feature_names(preprocessor):\n",
    "    # Get feature names from numeric features\n",
    "    numeric_features_out = numeric_features\n",
    "\n",
    "    # Get feature names from categorical features after encoding\n",
    "    cat_features = (preprocessor\n",
    "                   .named_transformers_['cat']\n",
    "                   .named_steps['onehot']\n",
    "                   .get_feature_names_out(categorical_features))\n",
    "    \n",
    "    # Combine both feature sets\n",
    "    return numeric_features_out + list(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2701b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full pipeline\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Prepare data\n",
    "X = penguins.drop(['species'], axis=1)\n",
    "y = penguins['species']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb42f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"\\nModel Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=penguins['species'].unique(),\n",
    "            yticklabels=penguins['species'].unique())\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Get feature names after encoding\n",
    "feature_names = get_feature_names(preprocessor)\n",
    "print(\"\\nEncoded Feature Names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5374230",
   "metadata": {},
   "source": [
    "A (risky) method to deal with potential outliers: quartile capping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(df, columns, lower_percentile=1, upper_percentile=99):\n",
    "    df_capped = df.copy()\n",
    "    for column in columns:\n",
    "        lower = np.percentile(df[column].dropna(), lower_percentile)\n",
    "        upper = np.percentile(df[column].dropna(), upper_percentile)\n",
    "        df_capped[column] = df_capped[column].clip(lower=lower, upper=upper)\n",
    "    return df_capped\n",
    "\n",
    "# Get numerical columns from features only (excluding target)\n",
    "numerical_cols_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Apply outlier capping\n",
    "X_capped = cap_outliers(X, numerical_cols_features)\n",
    "\n",
    "# Create pipeline with outlier capping and proper preprocessing\n",
    "capped_pipe = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('num', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('imputer', KNNImputer(n_neighbors=5))\n",
    "        ]), numeric_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(\n",
    "                drop='first',\n",
    "                sparse_output=False,\n",
    "                handle_unknown='ignore'\n",
    "            ))\n",
    "        ]), categorical_features)\n",
    "    ])),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Evaluate with outlier capping\n",
    "X_train_capped, X_test_capped, y_train, y_test = train_test_split(\n",
    "    X_capped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "capped_pipe.fit(X_train_capped, y_train)\n",
    "y_pred_capped = capped_pipe.predict(X_test_capped)\n",
    "\n",
    "# Evaluate results with visualization\n",
    "print(\"\\nResults with Outlier Capping:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_capped))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_capped, zero_division=0))\n",
    "\n",
    "# Visualize confusion matrix for capped results\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm_capped = confusion_matrix(y_test, y_pred_capped)\n",
    "sns.heatmap(cm_capped, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - With Outlier Capping')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f138985",
   "metadata": {},
   "source": [
    "#### Challenge (Workshop)\n",
    "1. Use the following code stub to perform the same task on the Cleveland Heart Disease dataset. Test the impact of each imputation strategy in the model performance. \n",
    "2. Compare the performance of Random Forests vs. XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626cf55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
    "           'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
    "df = pd.read_csv(url, names=columns, na_values='?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
